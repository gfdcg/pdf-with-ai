 PDF avec IA - Syst√®me RAG
Un syst√®me de Retrieval-Augmented Generation (RAG) puissant pour analyser et interroger des documents PDF avec l'intelligence artificielle.
 Fonctionnalit√©s

 Extraction intelligente de texte depuis des PDFs
 Analyse IA avanc√©e avec mod√®les de langage
 Interface de questions-r√©ponses intuitive
 Recherche s√©mantique dans les documents
 API REST pour int√©gration
 Interface web Streamlit moderne
 Support Docker pour d√©ploiement facile

üèó Architecture
pdf-with-ai/
 rag_api.py              
 streamlit_app.py        
 utils.py               
docker-compose.yml     
 Dockerfile            
 Dockerfile.streamlit   
 requirements.txt      
Installation et Utilisation
M√©thode 1: Installation locale
bash# 1. Cloner le projet
git clone https://github.com/gfdcg/pdf-with-ai.git
cd pdf-with-ai

2. Cr√©er un environnement virtuel
python -m venv rag_env
rag_env\Scripts\activate  # Windows

 3. Installer les d√©pendances
pip install -r requirements.txt

 4. Lancer l'API
python rag_api.py

5. Ou lancer l'interface Streamlit
streamlit run streamlit_app.py
M√©thode 2: Avec Docker (Recommand√©)
bash# Lancer tous les services
docker-compose up -d

# Acc√©der aux services
# - API: http://localhost:8000
# - Interface web: http://localhost:8501
Utilisation
Via l'interface web

Ouvrez http://localhost:8501
Uploadez votre PDF
Posez vos questions en langage naturel
Obtenez des r√©ponses contextuelles

Via l'API REST
pythonimport requests

Upload d'un PDF
files = {'file': open('document.pdf', 'rb')}
response = requests.post('http://localhost:8000/upload', files=files)

Poser une question
data = {'question': 'Quel est le sujet principal de ce document ?'}
response = requests.post('http://localhost:8000/ask', json=data)
print(response.json())
Programmation directe
pythonfrom rag_api import RAGSystem

 Initialiser le syst√®me
rag = RAGSystem()

# Analyser un PDF
rag.process_pdf("document.pdf")

 Poser une question
answer = rag.ask_question("R√©sume le contenu principal")
print(answer)
Configuration
Variables d'environnement (.env)
env# Mod√®le de langage
MODEL_NAME=llama2
OLLAMA_URL=http://localhost:11434

 Base de donn√©es vectorielle
VECTOR_DB_PATH=./data/vector_db
 API
API_HOST=0.0.0.0
API_PORT=8000
Pr√©requis

 Python 3.8+
 Ollama (pour les mod√®les locaux)
Librairies: PyTorch, Transformers, Streamlit
 Espace disque: ~2GB pour les mod√®les

 Scripts de d√©ploiement
Configuration automatique
bash# Rendre le script ex√©cutable
chmod +x setup.sh
 Lancer l'installation compl√®te
./setup.sh
Fix Docker (si n√©cessaire)
bashchmod +x fix_docker.sh
./fix_docker.sh
 Endpoints API
M√©thodeEndpointDescriptionPOST/uploadUpload et traitement d'un PDFPOST/askPoser une question sur le documentGET/healthV√©rification de l'√©tat du serviceGET/docsDocumentation interactive
 Contribution

Fork le projet
Cr√©ez une branche feature (git checkout -b feature/nouvelle-fonctionnalite)
Commitez vos changements (git commit -am 'Ajout: nouvelle fonctionnalit√©')
Push vers la branche (git push origin feature/nouvelle-fonctionnalite)
Ouvrez une Pull Request

 D√©pannage
Probl√®mes courants
Erreur Ollama :
bash# V√©rifier si Ollama est d√©marr√©
ollama serve

# Installer un mod√®le
ollama pull llama2
Erreur de m√©moire :

R√©duisez la taille des chunks dans utils.py
Utilisez un mod√®le plus l√©ger

Port d√©j√† utilis√© :
bash# Changer les ports dans docker-compose.yml
# Ou arr√™ter les services existants
docker-compose down
